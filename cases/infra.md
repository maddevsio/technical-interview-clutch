## Описание кейса
Спроектировать сервис потоковой обработки.
## Задача
1. Сервис принимает тяжелые данные (json-serializable) (100+ MB) за раз (эндпоинт `/submit`) -> `input`
2. Данные должны быть обработаны в несколько шагов (каждый из шагов может вызывать внешний API):
- Обработка нескольких ключей (из `input`) независимо (пример: ключи `constraints`, `contracts`, `orders`)
- После предыдущего пункта результаты обработки `constraints`, `contracts` мерджятся в одну структуру и проводится дополнительная обработка (`contracts-constraints-result`)
- Последний шаг - обработка `contracts-constraints-result` и `orders` вместе и выдача результата -> `result`
3. Необходимо оптимизировать потребление памяти / CPU
4. Обработка данных должна быть идемпотентной.
5. Эндпоинт приема данных должен работать быстро (<10 seconds)
6. Метаданные из `input` должны храниться персистентно и отдаваться пользователю в эндпоинте

## На что обращаем внимание
1. `/submit` должен использовать потоковую обработку для экономии памяти, хотя и больше времени занимает
2. Можно использовать celery, airflow, spark, etc. Но надо проверить знание основ потоковой обработки
3. Для идемпотентности можно использовать очереди, БД, redis/memcached, etc. Надо у любого выбранного метода выяснить минусы/плюсы
4. Как передаются данные на каждом шагу? S3, Redis, DB?
5. Знаком ли с концепцией ETL/ELT?
6. Как кандидат будет мониторить?
7. Как будет запускаться сервис? k8s, docker compose, AWS Lambda, etc.?
## Пояснения
1. 
